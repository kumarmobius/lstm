name: LSTM ONNX v3
description: Converts a PyTorch LSTM model to ONNX by reading model and architecture from a JSON config string.
inputs:
  - {name: pth_file, type: Model, description: "Directory containing model.pth"}
  - {name: config, type: String, description: "JSON model configuration (see example below)"}
  - {name: output_filename, type: String, default: "lstm_model.onnx", description: "Output ONNX file name"}
outputs:
  - {name: onnx_model, type: Model, description: "Converted ONNX model"}
implementation:
  container:
    image: gurpreetgandhi/nesy-factory:v25
    command:
      - sh
      - -c
      - |
        PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet onnx onnxruntime || \
        PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet onnx onnxruntime --user
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import argparse
        import json
        import pickle
        import os
        import sys
        import torch
        import torch.onnx
        import onnx
        import onnxruntime as ort

        # Import your model class
        from nesy_factory.RNNs import LSTM

        parser = argparse.ArgumentParser()
        parser.add_argument('--pth_file', type=str, required=True)
        parser.add_argument('--config', type=str, required=True)
        parser.add_argument('--output_filename', type=str, default='lstm_model.onnx')
        parser.add_argument('--onnx_model', type=str, required=True)
        args = parser.parse_args()

        pth_path = os.path.join(args.pth_file, "model.pth")

        if not os.path.exists(pth_path):
            print(f"ERROR: model.pth not found at {pth_path}")
            sys.exit(1)

        # -------------------------
        # Parse JSON config (single source of truth)
        # -------------------------
        print("Parsing JSON config...")
        try:
            cfg = json.loads(args.config)
        except Exception as e:
            print("ERROR: Failed to parse JSON config:", e)
            sys.exit(1)

        print("Loaded config:")
        print(json.dumps(cfg, indent=2))

        # Required fields
        seq_len = cfg.get("seq_length")
        feature_dim = cfg.get("processed_input_dim") or cfg.get("input_dim")

        if seq_len is None or feature_dim is None:
            print("ERROR: JSON config must include 'seq_length' and 'processed_input_dim' (or 'input_dim')")
            sys.exit(1)

        try:
            seq_len = int(seq_len)
            feature_dim = int(feature_dim)
        except Exception:
            print("ERROR: seq_length and processed_input_dim must be integers")
            sys.exit(1)

        # Build model_config from JSON
        model_config = {
            'input_dim': feature_dim,
            'hidden_dim': cfg.get('hidden_dim', 32),
            'output_dim': cfg.get('output_dim', 1),
            'num_layers': cfg.get('num_layers', 1),
            'dropout': cfg.get('dropout', 0.0),
            'optimizer': cfg.get('optimizer', 'adam'),
            'learning_rate': cfg.get('learning_rate', 0.001),
            'epochs': cfg.get('epochs', 1),
            'loss_function': cfg.get('loss_function', 'BCEWithLogitsLoss')
        }

        print("Model config constructed:")
        print(json.dumps(model_config, indent=2))

        # Output directory
        os.makedirs(args.onnx_model, exist_ok=True)
        output_path = os.path.join(args.onnx_model, args.output_filename)

        # ---------------------------
        # Instantiate / load model
        # ---------------------------
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        print("Attempting to load pickled model from:", pth_path)
        with open(pth_path, 'rb') as f:
            loaded_obj = pickle.load(f)

        # If the pickle contains the full model object, use it
        if isinstance(loaded_obj, LSTM):
            model = loaded_obj
            print("Loaded full LSTM model object from pickle.")
        else:
            # Otherwise assume it's a state_dict and rebuild the model
            print("Pickle does not contain full model object. Rebuilding LSTM from model_config...")
            model = LSTM(model_config)
            try:
                model.load_state_dict(loaded_obj, strict=False)
                print("Loaded state_dict into reconstructed model.")
            except Exception as e:
                print("ERROR: Failed to load state_dict into LSTM:", e)
                print("If the pickle contains a different object, you must provide a compatible state_dict or a full model pickle.")
                sys.exit(1)

        model.to(device)
        model.eval()

        # Dummy input: shape [batch, seq_len, feature_dim]
        batch_size = 1
        dummy = torch.randn(batch_size, seq_len, feature_dim, dtype=torch.float32).to(device)

        print("Exporting to ONNX at:", output_path)
        try:
            torch.onnx.export(
                model,
                dummy,
                output_path,
                input_names=["INPUT__0"],
                output_names=["OUTPUT__0"],
                dynamic_axes={
                    "INPUT__0": {0: "batch", 1: "seq_len"},
                    "OUTPUT__0": {0: "batch"}
                },
                opset_version=17,
                do_constant_folding=True,
            )
        except Exception as e:
            print("ERROR: ONNX export failed:", e)
            sys.exit(1)

        print("Validating ONNX model...")
        try:
            onnx_model = onnx.load(output_path)
            onnx.checker.check_model(onnx_model)
            print("ONNX validation OK.")
        except Exception as e:
            print("ERROR: ONNX validation failed:", e)
            sys.exit(1)

        print("Running ONNX Runtime smoke test...")
        try:
            session = ort.InferenceSession(output_path)
            input_name = session.get_inputs()[0].name
            output_name = session.get_outputs()[0].name
            out = session.run([output_name], {input_name: dummy.cpu().numpy()})
            print("Smoke test OK. Output shape:", out[0].shape)
        except Exception as e:
            print("ERROR: ONNX Runtime inference failed:", e)
            sys.exit(1)

        print("Saved ONNX model to:", output_path)
    args:
      - --pth_file
      - {inputPath: pth_file}
      - --config
      - {inputValue: config}
      - --output_filename
      - {inputValue: output_filename}
      - --onnx_model
      - {outputPath: onnx_model}
