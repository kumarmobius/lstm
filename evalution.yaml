name: Eval LSTM
inputs:
  - name: trained_model
    type: Model
    description: "Trained model checkpoint (torch-saved file/dir or pickle file)"

  - name: test_loader
    type: Dataset
    description: "Torch-saved test DataLoader / iterable (saved with torch.save) or pickle"

  - name: config_json
    type: String
    description: "Either a path to a JSON file or a raw JSON string containing model config"

outputs:
  - name: metrics_json
    type: String
    description: "Evaluation metrics saved as JSON"

implementation:
  container:
    image: gurpreetgandhi/nesy-factory:v25
    command:
      - python3
      - -u
      - -c
      - |
          import argparse, json, os, sys, traceback, tempfile, time
          import torch, pickle, math
          import numpy as np

          def ensure_parent_dir(path):
              parent = os.path.dirname(path)
              if parent:
                  os.makedirs(parent, exist_ok=True)

          def atomic_write_json(path, data):
              ensure_parent_dir(path)
              dirp = os.path.dirname(path) or "/tmp"
              fd, tmp = tempfile.mkstemp(prefix="tmp_json_", dir=dirp)
              try:
                  with os.fdopen(fd, "w") as f:
                      json.dump(data, f)
                  os.replace(tmp, path)
              except Exception:
                  try:
                      os.remove(tmp)
                  except Exception:
                      pass
                  raise

          def find_actual_file(path):
              if os.path.isfile(path):
                  return path
              data_path = os.path.join(path, "data")
              if os.path.isfile(data_path):
                  return data_path
              if os.path.isdir(path):
                  candidates = []
                  for root, _, files in os.walk(path):
                      for fn in files:
                          if fn.startswith('.'):
                              continue
                          candidates.append(os.path.join(root, fn))
                  if not candidates:
                      raise FileNotFoundError(f"No files found inside directory: {path}")
                  for ext in (".pth", ".pt", ".pkl", ".bin"):
                      for c in candidates:
                          if c.lower().endswith(ext):
                              return c
                  return sorted(candidates)[0]
              return path

          def load_with_fallback(path):
              actual = find_actual_file(path)
              print(f"[eval] Loading object from: {actual}", file=sys.stdout)
              try:
                  obj = torch.load(actual, map_location="cpu")
                  print("[eval] torch.load succeeded for:", actual, file=sys.stdout)
                  return obj
              except Exception as e_torch:
                  print("[eval] torch.load failed:", repr(e_torch), file=sys.stdout)
                  print("[eval] Attempting pickle.load fallback...", file=sys.stdout)
                  try:
                      with open(actual, "rb") as f:
                          obj = pickle.load(f)
                      print("[eval] pickle.load succeeded for:", actual, file=sys.stdout)
                      return obj
                  except Exception as e_pickle:
                      print("[eval] pickle.load also failed:", repr(e_pickle), file=sys.stdout)
                      raise RuntimeError(f"Failed to load file: {actual} torch_error: {e_torch} pickle_error: {e_pickle}")

          def parse_config_input(config_input):
              try:
                  if os.path.exists(config_input):
                      with open(config_input, "r") as f:
                          return json.load(f)
              except Exception:
                  pass
              try:
                  return json.loads(config_input)
              except Exception as e:
                  raise ValueError(f"config_json is neither a valid file path nor a valid JSON string: {e}")

          parser = argparse.ArgumentParser(description="Robust Eval LSTM")
          parser.add_argument("--trained_model", required=True)
          parser.add_argument("--test_loader", required=True)
          parser.add_argument("--config_json", required=True, help="Path or raw JSON string")
          parser.add_argument("--metrics_json", required=True)
          args = parser.parse_args()

          print("[eval] Args:", args, file=sys.stdout)

          try:
              config = parse_config_input(args.config_json)
              print("[eval] Parsed config keys:", list(config.keys()), file=sys.stdout)

              # ensure input_dim
              if "input_dim" not in config:
                  if "processed_input_dim" in config:
                      config["input_dim"] = int(config["processed_input_dim"])
                      print("[eval] auto-filled input_dim from processed_input_dim:", config["input_dim"], file=sys.stdout)
                  elif "feature_columns" in config and isinstance(config["feature_columns"], (list, tuple)):
                      config["input_dim"] = int(len(config["feature_columns"]))
                      print("[eval] auto-filled input_dim from feature_columns length:", config["input_dim"], file=sys.stdout)
                  else:
                      raise KeyError("Missing 'input_dim' in config. Provide input_dim or processed_input_dim or feature_columns.")

              # ensure output_dim
              if "output_dim" not in config:
                  loss_fn = str(config.get("loss_function","")).lower()
                  if "bce" in loss_fn or "binary" in loss_fn:
                      config["output_dim"] = 1
                      print("[eval] auto-filled output_dim = 1 (binary)", file=sys.stdout)
                  elif "num_classes" in config:
                      config["output_dim"] = int(config["num_classes"])
                      print("[eval] auto-filled output_dim from num_classes", file=sys.stdout)
                  else:
                      config["output_dim"] = 1
                      print("[eval] defaulted output_dim = 1", file=sys.stdout)

              # threshold for binary predictions (config override allowed)
              threshold = float(config.get("threshold", 0.5))

              # load checkpoint
              checkpoint = load_with_fallback(args.trained_model)
              model_state = None
              model_instance = None
              if isinstance(checkpoint, dict):
                  if "model_state" in checkpoint:
                      model_state = checkpoint["model_state"]
                  elif "state_dict" in checkpoint:
                      model_state = checkpoint["state_dict"]
                  else:
                      keys = list(checkpoint.keys())
                      if keys and isinstance(keys[0], str) and '.' in keys[0]:
                          model_state = checkpoint
                      else:
                          for k in ("model","net","module"):
                              if k in checkpoint and hasattr(checkpoint[k], "__class__"):
                                  model_instance = checkpoint[k]
                                  break
              else:
                  model_instance = checkpoint

              # import LSTM and create model
              from nesy_factory.RNNs import LSTM
              model = LSTM(config)

              # load weights if present
              if model_instance is not None:
                  try:
                      model.load_state_dict(model_instance.state_dict(), strict=False)
                      print("[eval] Loaded state from saved model instance", file=sys.stdout)
                  except Exception as e:
                      print("[eval] Warning loading from model instance:", e, file=sys.stdout)
              elif model_state is not None:
                  new_state = {}
                  for k,v in model_state.items():
                      if isinstance(k, str) and k.startswith("module."):
                          new_state[k.replace("module.","",1)] = v
                      else:
                          new_state[k] = v
                  try:
                      model.load_state_dict(new_state, strict=False)
                      print("[eval] Loaded state_dict into model", file=sys.stdout)
                  except Exception as e:
                      print("[eval] Warning loading state_dict:", e, file=sys.stdout)
              else:
                  print("[eval] No model weights found; using fresh model instance", file=sys.stdout)

              # ensure loss function
              loss_name = str(config.get("loss_function","")).lower()
              if "bce" in loss_name:
                  loss_fn = torch.nn.BCEWithLogitsLoss()
                  is_binary = True
              elif "cross" in loss_name:
                  loss_fn = torch.nn.CrossEntropyLoss()
                  is_binary = False
              elif "mse" in loss_name or "mse" == loss_name:
                  loss_fn = torch.nn.MSELoss()
                  is_binary = False
              else:
                  # default to BCE for safety if output_dim==1
                  if config.get("output_dim",1) == 1:
                      loss_fn = torch.nn.BCEWithLogitsLoss(); is_binary = True
                  else:
                      loss_fn = torch.nn.MSELoss(); is_binary = False

              device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
              model.to(device)
              model.eval()

              # load test loader
              test_obj = load_with_fallback(args.test_loader)
              print("[eval] test object type:", type(test_obj), file=sys.stdout)

              # iterate and collect preds and labels
              all_logits = []
              all_probs = []
              all_preds = []
              all_labels = []
              total_loss = 0.0
              total_n = 0

              with torch.no_grad():
                  for batch in test_obj:
                      try:
                          xb, yb = batch
                      except Exception:
                          # unsupported batch form
                          continue
                      xb = xb.to(device).float()
                      yb_cpu = yb.detach().cpu()
                      # forward
                      out = model(xb)
                      out_cpu = out.detach().cpu()
                      # compute loss (match shapes)
                      try:
                          # adapt shapes
                          if isinstance(loss_fn, torch.nn.BCEWithLogitsLoss):
                              yb_t = yb_cpu.float()
                              if out_cpu.dim() > yb_t.dim():
                                  out_proc = out_cpu.squeeze(-1)
                              else:
                                  out_proc = out_cpu
                              loss = loss_fn(out_proc, yb_t.to(out_proc.dtype))
                          elif isinstance(loss_fn, torch.nn.CrossEntropyLoss):
                              loss = loss_fn(out_cpu, yb_cpu.long())
                          else:
                              # regression
                              yb_t = yb_cpu.float()
                              if out_cpu.dim() > yb_t.dim():
                                  out_proc = out_cpu.squeeze(-1)
                              else:
                                  out_proc = out_cpu
                              loss = loss_fn(out_proc, yb_t.to(out_proc.dtype))
                      except Exception as e:
                          loss = 0.0
                          print("[eval] Warning: loss compute failed for batch:", e, file=sys.stdout)

                      # collect
                      batch_n = int(np.prod(yb_cpu.shape))
                      total_loss += float(loss) * batch_n
                      total_n += batch_n

                      # Build labels array in 1D form
                      yb_arr = yb_cpu.numpy()
                      # if one-hot, convert to indices
                      if yb_arr.ndim > 1 and yb_arr.shape[1] > 1:
                          yb_proc = np.argmax(yb_arr, axis=1)
                      else:
                          yb_proc = yb_arr.reshape(-1)

                      # predictions handling
                      if is_binary:
                          # logits -> probs -> threshold -> binary preds
                          # ensure out_cpu is 1D
                          if out_cpu.dim() > 1 and out_cpu.shape[1] == 1:
                              logits = out_cpu.view(-1).numpy()
                          elif out_cpu.dim() == 2 and out_cpu.shape[1] > 1:
                              # multi-output - reduce? take first column (best-effort)
                              logits = out_cpu[:,0].numpy()
                          else:
                              logits = out_cpu.reshape(-1).numpy()
                          probs = 1.0 / (1.0 + np.exp(-logits))
                          preds = (probs >= threshold).astype(int)
                      else:
                          # multiclass/regression: if out dimension >1 treat as class logits
                          if out_cpu.dim() == 2 and out_cpu.shape[1] > 1:
                              probs_arr = torch.softmax(out_cpu, dim=1).numpy()
                              preds = np.argmax(probs_arr, axis=1)
                              probs = np.max(probs_arr, axis=1)
                              logits = None
                          else:
                              # regression numeric output
                              preds = out_cpu.reshape(-1).numpy()
                              probs = preds.copy()
                              logits = preds.copy()

                      # append
                      all_labels.append(yb_proc)
                      all_preds.append(preds)
                      all_probs.append(probs)
                      if 'logits' in locals() and logits is not None:
                          all_logits.append(logits)

              if total_n > 0:
                  avg_loss = total_loss / float(total_n)
              else:
                  avg_loss = None

              if len(all_labels) == 0:
                  raise RuntimeError("No evaluation samples found in test_loader.")

              y_all = np.concatenate(all_labels).ravel()
              p_all = np.concatenate(all_preds).ravel()
              prob_all = np.concatenate(all_probs).ravel()

              # normalize labels to integers when appropriate
              # if labels are floats in [0,1], convert via threshold 0.5
              if set(np.unique(y_all)).issubset(set([0.0,1.0])) or set(np.unique(y_all)).issubset(set([0,1])):
                  label_is_binary = True
              else:
                  label_is_binary = False

              # If label is float continuous but predictions binary -> try to coerce labels
              if label_is_binary and (prob_all.dtype.kind in 'f'):
                  # OK
                  pass

              # compute metrics - store in temp variables
              computed_accuracy = None
              
              # try sklearn, otherwise fallback
              try:
                  from sklearn.metrics import precision_recall_fscore_support, accuracy_score, roc_auc_score
                  if is_binary:
                      precision, recall, f1, _ = precision_recall_fscore_support(y_all.astype(int), p_all.astype(int), average='binary', zero_division=0)
                      computed_accuracy = float(accuracy_score(y_all.astype(int), p_all.astype(int)))
                      print(f"[eval] Computed metrics - accuracy: {computed_accuracy}, precision: {precision}, recall: {recall}, f1: {f1}", file=sys.stdout)
                  else:
                      # multiclass / regression: try macro averaging if preds are discrete
                      if p_all.dtype.kind in 'f' and np.all(np.mod(p_all,1)==0):
                          p_all_int = p_all.astype(int)
                          precision, recall, f1, _ = precision_recall_fscore_support(y_all.astype(int), p_all_int, average='macro', zero_division=0)
                          computed_accuracy = float(accuracy_score(y_all.astype(int), p_all_int))
                          print(f"[eval] Computed metrics - accuracy: {computed_accuracy}, precision: {precision}, recall: {recall}, f1: {f1}", file=sys.stdout)
                      else:
                          # regression metrics fallback
                          print("[eval] Regression mode - no accuracy metric", file=sys.stdout)
              except Exception as e_sk:
                  print("[eval] sklearn metrics failed or unavailable:", e_sk, file=sys.stdout)
                  # manual safe computations
                  try:
                      yb_int = y_all.astype(int)
                      p_int = p_all.astype(int)
                      tp = int(((p_int==1) & (yb_int==1)).sum())
                      fp = int(((p_int==1) & (yb_int==0)).sum())
                      tn = int(((p_int==0) & (yb_int==0)).sum())
                      fn = int(((p_int==0) & (yb_int==1)).sum())
                      precision = tp / (tp+fp) if (tp+fp)>0 else 0.0
                      recall = tp / (tp+fn) if (tp+fn)>0 else 0.0
                      acc = (tp+tn) / max(1, (tp+tn+fp+fn))
                      f1 = (2*precision*recall/(precision+recall)) if (precision+recall)>0 else 0.0
                      computed_accuracy = float(acc)
                      print(f"[eval] Fallback computed metrics - accuracy: {computed_accuracy}, precision: {precision}, recall: {recall}, f1: {f1}", file=sys.stdout)
                  except Exception as e2:
                      print("[eval] Fallback manual metric computation failed:", e2, file=sys.stdout)

              # Create final metrics JSON with timestamp, loss*100, accuracy*100
              final_metrics = {
                  "timestamp": int(time.time()),
                  "loss": float(avg_loss * 100) if avg_loss is not None else None,
                  "accuracy": float(computed_accuracy * 100) if computed_accuracy is not None else None
              }

              print("[eval] Final metrics:", final_metrics, file=sys.stdout)

              # write metrics
              atomic_write_json(args.metrics_json, final_metrics)
              print("[eval] Wrote metrics to:", args.metrics_json, file=sys.stdout)

          except Exception as e:
              print("[eval] ERROR during setup/eval:", e, file=sys.stdout)
              traceback.print_exc()
              try:
                  ensure_parent_dir(args.metrics_json)
                  with open(args.metrics_json, "w") as f:
                      json.dump({"timestamp": int(time.time()), "error": str(e)}, f)
              except Exception:
                  pass
              sys.exit(1)
    args:
      - --trained_model
      - { inputPath: trained_model }
      - --test_loader
      - { inputPath: test_loader }
      - --config_json
      - { inputValue: config_json }
      - --metrics_json
      - { outputPath: metrics_json }
