name: Eval RNN Model v3
inputs:
  - name: trained_model
    type: Model
    description: "Trained model checkpoint containing model_state (torch-saved file or folder)"

  - name: test_loader
    type: Dataset
    description: "Torch-saved test DataLoader / iterable (saved with torch.save)"

  - name: config_json
    type: String
    description: "JSON file containing model config for instantiating LSTM (external input)"

outputs:
  - name: metrics_json
    type: Metrics
    description: "Evaluation metrics saved as JSON"

implementation:
  container:
    image: gurpreetgandhi/nesy-factory:v25
    command:
      - python3
      - -u
      - -c
      - |
          import argparse
          import json
          import os
          import sys
          import traceback
          import torch
          import tempfile

          def ensure_parent_dir(path):
              parent = os.path.dirname(path)
              if parent:
                  os.makedirs(parent, exist_ok=True)

          def atomic_write_json(path, data):
              ensure_parent_dir(path)
              dirp = os.path.dirname(path) or "/tmp"
              fd, tmp = tempfile.mkstemp(prefix="tmp_json_", dir=dirp)
              try:
                  with os.fdopen(fd, "w") as f:
                      json.dump(data, f)
                  os.replace(tmp, path)
              except Exception:
                  try:
                      os.remove(tmp)
                  except Exception:
                      pass
                  raise

          parser = argparse.ArgumentParser(description="Evaluate LSTM model using nesy_factory.RNNs.LSTM")
          parser.add_argument("--trained_model", required=True, help="Path to trained model checkpoint (torch-saved)")
          parser.add_argument("--test_loader", required=True, help="Path to torch-saved test DataLoader/iterable")
          parser.add_argument("--config_json", required=True, help="Path to external config JSON file")
          parser.add_argument("--metrics_json", required=True, help="Output path for metrics JSON (file)")
          args = parser.parse_args()

          print("[eval] Args:", args, file=sys.stdout)

          try:
              # 1) load config
              print(f"[eval] Loading config from: {args.config_json}", file=sys.stdout)
              with open(args.config_json, "r") as f:
                  config = json.load(f)
              print("[eval] Config keys:", list(config.keys()), file=sys.stdout)

              # 2) load checkpoint
              print(f"[eval] Loading checkpoint from: {args.trained_model}", file=sys.stdout)
              ckpt_path = args.trained_model
              # try torch.load and fallback behavior for directories (allow 'data' inside)
              def find_actual_file(path):
                  if os.path.isfile(path):
                      return path
                  data_path = os.path.join(path, "data")
                  if os.path.isfile(data_path):
                      return data_path
                  if os.path.isdir(path):
                      candidates = []
                      for root, _, files in os.walk(path):
                          for fn in files:
                              if fn.startswith('.'):
                                  continue
                              candidates.append(os.path.join(root, fn))
                      if not candidates:
                          raise FileNotFoundError(f"No files found inside directory: {path}")
                      # prefer common extensions
                      for ext in (".pth", ".pt", ".pkl", ".bin"):
                          for c in candidates:
                              if c.lower().endswith(ext):
                                  return c
                      return sorted(candidates)[0]
                  return path

              ckpt_actual = find_actual_file(ckpt_path)
              print(f"[eval] Resolved checkpoint file: {ckpt_actual}", file=sys.stdout)
              checkpoint = torch.load(ckpt_actual, map_location="cpu")
              print(f"[eval] Checkpoint type: {type(checkpoint)}", file=sys.stdout)

              # checkpoint must contain model_state (state dict) or possibly be the model itself
              model_state = None
              model_instance = None
              if isinstance(checkpoint, dict):
                  if "model_state" in checkpoint:
                      model_state = checkpoint["model_state"]
                  elif "state_dict" in checkpoint:
                      model_state = checkpoint["state_dict"]
                  else:
                      # maybe the dict is raw state_dict (heuristic)
                      keys = list(checkpoint.keys())
                      if keys and isinstance(keys[0], str) and '.' in keys[0]:
                          model_state = checkpoint
                      # or it might contain a saved model object under some key
                      for k in ("model", "net", "module"):
                          if k in checkpoint and hasattr(checkpoint[k], "__class__"):
                              model_instance = checkpoint[k]
                              break
              else:
                  # checkpoint might be a saved nn.Module instance if torch.save(model, path) used
                  model_instance = checkpoint

              # 3) import LSTM and instantiate with external config
              print("[eval] Importing LSTM from nesy_factory.RNNs ...", file=sys.stdout)
              from nesy_factory.RNNs import LSTM

              print("[eval] Creating model instance using external config ...", file=sys.stdout)
              model = LSTM(config)

              # if we found saved model instance, try to copy state, otherwise load state_dict if present
              if model_instance is not None:
                  try:
                      # if model_instance is an nn.Module, try to copy weights
                      model.load_state_dict(model_instance.state_dict(), strict=False)
                      print("[eval] Loaded weights from model instance in checkpoint.", file=sys.stdout)
                  except Exception as e:
                      print("[eval] Warning: could not load weights from saved instance:", e, file=sys.stdout)
              elif model_state is not None:
                  # strip module. prefix if present
                  new_state = {}
                  for k, v in model_state.items():
                      if k.startswith("module."):
                          new_state[k.replace("module.", "", 1)] = v
                      else:
                          new_state[k] = v
                  try:
                      model.load_state_dict(new_state, strict=False)
                      print("[eval] Loaded state_dict into model (strict=False).", file=sys.stdout)
                  except Exception as e:
                      print("[eval] Warning: loading state_dict failed:", e, file=sys.stdout)
              else:
                  print("[eval] WARNING: no model_state or model instance found in checkpoint; proceeding with freshly initialized model.", file=sys.stdout)

              # ensure optimizer & criterion exist on model (BaseRNN helper)
              try:
                  model._init_optimizer_and_criterion()
              except Exception as e:
                  print("[eval] Warning: model._init_optimizer_and_criterion() failed:", e, file=sys.stdout)

              # 4) load test loader
              print(f"[eval] Loading test loader from: {args.test_loader}", file=sys.stdout)
              test_path = args.test_loader
              test_actual = find_actual_file(test_path)
              test_obj = torch.load(test_actual, map_location="cpu")

              # model.eval_step expects an iterable/test dataloader saved via torch.save
              print("[eval] Running eval_step() ...", file=sys.stdout)
              metrics = model.eval_step(test_obj)

              # normalize metrics values to native python floats where possible
              def _norm(v):
                  if v is None:
                      return None
                  try:
                      return float(v)
                  except Exception:
                      return v
              metrics = {k: (_norm(v) if not isinstance(v, (dict, list)) else v) for k, v in metrics.items()}

              print("[eval] Metrics computed:", metrics, file=sys.stdout)

              # 5) write metrics JSON atomically to output path
              print(f"[eval] Writing metrics to: {args.metrics_json}", file=sys.stdout)
              atomic_write_json = atomic_write_json if 'atomic_write_json' in globals() else None
          except Exception as e:
              print("[eval] ERROR during setup/eval:", e, file=sys.stdout)
              traceback.print_exc()
              # still attempt to write an error JSON for debugging
              try:
                  err_out = {"error": str(e)}
                  ensure_parent_dir(args.metrics_json)
                  with open(args.metrics_json, "w") as f:
                      json.dump(err_out, f)
              except Exception:
                  pass
              sys.exit(1)

          try:
              # write metrics to provided output path
              ensure_parent_dir(args.metrics_json)
              with open(args.metrics_json, "w") as f:
                  json.dump(metrics, f, indent=2)
              print(f"[eval] Successfully wrote metrics to: {args.metrics_json}", file=sys.stdout)
          except Exception as e:
              print("[eval] ERROR writing metrics JSON:", e, file=sys.stdout)
              traceback.print_exc()
              sys.exit(1)
    args:
      - --trained_model
      - { inputPath: trained_model }
      - --test_loader
      - { inputPath: test_loader }
      - --config_json
      - { inputPath: config_json }
      - --metrics_json
      - { inputValue: metrics_json }
