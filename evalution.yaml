name: Evaluate LSTM v1
inputs:
  - {name: test_loader, type: Dataset}
  - {name: model_config, type: String}
  - {name: trained_model, type: Model}
outputs:
  - {name: metrics_json, type: String}
implementation:
  container:
    image: gurpreetgandhi/nesy-factory:v25
    command:
      - python3
      - -u
      - -c
      - |
        import argparse, os, sys, json, tempfile, tempfile, traceback, importlib
        import torch
        import torch.nn as nn
        import numpy as np
        from torch.utils.data import TensorDataset, DataLoader

        parser = argparse.ArgumentParser(description="Evaluate saved model on test_loader")
        parser.add_argument("--test_loader", required=True, help="Path to torch-saved test dataset / iterable")
        parser.add_argument("--model_config", required=False, default="", help="JSON string with eval config (optional)")
        parser.add_argument("--trained_model", required=True, help="Path to saved model object or checkpoint")
        parser.add_argument("--metrics_json", required=True, help="Output JSON path for metrics")
        args = parser.parse_args()

        def ensure_parent_dir(path):
            parent = os.path.dirname(path)
            if parent:
                os.makedirs(parent, exist_ok=True)

        def atomic_write_json(path, data):
            ensure_parent_dir(path)
            dirp = os.path.dirname(path) or "/tmp"
            fd, tmp = tempfile.mkstemp(prefix="tmp_json_", dir=dirp)
            try:
                with os.fdopen(fd, "w") as f:
                    json.dump(data, f)
                os.replace(tmp, path)
            except Exception:
                try:
                    os.remove(tmp)
                except Exception:
                    pass
                raise

        def find_actual_file(path, prefer_exts=(".pth", ".pt", ".pkl", ".bin")):
            if os.path.isfile(path):
                return path
            data_path = os.path.join(path, "data")
            if os.path.isfile(data_path):
                return data_path
            if os.path.isdir(path):
                candidates = []
                for root, dirs, files in os.walk(path):
                    for f in files:
                        if f.startswith('.'):
                            continue
                        candidates.append(os.path.join(root, f))
                if not candidates:
                    raise FileNotFoundError(f"No files found inside directory: {path}")
                for ext in prefer_exts:
                    for c in candidates:
                        if c.lower().endswith(ext):
                            return c
                candidates = sorted(candidates)
                return candidates[0]
            return path

        def load_torch_obj(path):
            actual_path = find_actual_file(path)
            print(f"[eval] Loading from: {actual_path}")
            if not os.path.exists(actual_path):
                raise FileNotFoundError(actual_path)
            try:
                return torch.load(actual_path, map_location='cpu')
            except Exception as e:
                # fallback to pickle
                import pickle
                with open(actual_path, 'rb') as f:
                    return pickle.load(f)

        def try_instantiate_from_spec(spec_str, init_args):
            # spec_str expected as "module.submodule:ClassName"
            if not spec_str:
                return None
            try:
                module_path, cls_name = spec_str.split(":")
                module = importlib.import_module(module_path)
                cls = getattr(module, cls_name)
                return cls(**(init_args or {}))
            except Exception as e:
                print("[eval] Failed to import/instantiate model class from spec:", spec_str, e)
                return None

        def iterable_from(obj):
            if hasattr(obj, '__iter__') and not isinstance(obj, dict):
                return obj
            raise RuntimeError("Provided test object is not iterable - expected DataLoader/list/TensorDataset saved via torch.save")

        # --- parse config ---
        eval_config = {}
        if args.model_config and args.model_config.strip():
            try:
                eval_config = json.loads(args.model_config)
                print("[eval] loaded model_config keys:", list(eval_config.keys()))
            except Exception as e:
                print("[eval] Warning: failed to parse model_config JSON string:", e)
                eval_config = {}

        loss_name = eval_config.get('loss_function', 'BCEWithLogitsLoss')
        device_req = eval_config.get('device', None)
        threshold = float(eval_config.get('threshold', 0.5))

        # --- load trained model (multiple fallback strategies) ---
        try:
            trained_obj = load_torch_obj(args.trained_model)
        except Exception as e:
            print("[eval] ERROR loading trained_model:", e)
            traceback.print_exc()
            sys.exit(1)

        model = None
        state_dict = None

        # If loaded object is an nn.Module instance -> use it directly
        if isinstance(trained_obj, nn.Module):
            model = trained_obj
            print("[eval] Loaded model instance directly.")
        elif isinstance(trained_obj, dict):
            # common cases: {'model_state': ..., ...} or direct state_dict
            if 'model_state' in trained_obj:
                state_dict = trained_obj['model_state']
                print("[eval] Checkpoint contains 'model_state'.")
            elif 'state_dict' in trained_obj:
                state_dict = trained_obj['state_dict']
                print("[eval] Checkpoint contains 'state_dict'.")
            else:
                # might be a raw state_dict (keys look like param names)
                # Heuristic: if keys are strings with '.' assume state_dict
                keys = list(trained_obj.keys())
                if keys and isinstance(keys[0], str) and '.' in keys[0]:
                    state_dict = trained_obj
                    print("[eval] Loaded object looks like a raw state_dict.")
                else:
                    # maybe entire object wrapped; keep as dict (maybe contains model instance under key)
                    for k in ('model','net','module'):
                        if k in trained_obj and isinstance(trained_obj[k], nn.Module):
                            model = trained_obj[k]
                            print(f"[eval] Found model instance under key '{k}'.")
                            break
        else:
            print("[eval] Loaded trained_model is of unexpected type:", type(trained_obj))

        # If only state_dict present, try to instantiate model using model_class spec in model_config
        if model is None and state_dict is not None:
            model_spec = eval_config.get('model_class', "")
            model_init_args = eval_config.get('model_init_args', {})
            if model_spec:
                model_candidate = try_instantiate_from_spec(model_spec, model_init_args)
                if model_candidate is not None:
                    model = model_candidate
                    try:
                        # strip module. prefix if needed
                        new_state = {}
                        for k, v in state_dict.items():
                            if k.startswith('module.'):
                                new_state[k.replace('module.', '', 1)] = v
                            else:
                                new_state[k] = v
                        model.load_state_dict(new_state, strict=False)
                        print("[eval] Loaded state_dict into instantiated model.")
                    except Exception as e:
                        print("[eval] Failed to load state_dict into instantiated model:", e)
                        model = None
                else:
                    print("[eval] Could not instantiate model from model_class spec.")
            else:
                print("[eval] No 'model_class' provided in model_config to reconstruct model from state_dict.")

        if model is None and state_dict is None:
            # last resort: maybe the file was a checkpoint with a serialized model under some key (search)
            if isinstance(trained_obj, dict):
                for k, v in trained_obj.items():
                    if isinstance(v, nn.Module):
                        model = v
                        print(f"[eval] Found model instance under key '{k}'.")
                        break

        if model is None:
            print("[eval] ERROR: unable to obtain a torch.nn.Module instance for evaluation.")
            print("Either provide a saved model instance (torch.save(model, path)) as trained_model,")
            print("or include 'model_class' (module:ClassName) and 'model_init_args' in model_config so the checkpoint state_dict can be loaded.")
            sys.exit(1)

        # --- finalize device and move model ---
        device = torch.device(device_req) if device_req else (torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'))
        model.to(device)
        model.eval()

        # --- load test loader ---
        try:
            test_obj = load_torch_obj(args.test_loader)
            test_iter = iterable_from(test_obj)
        except Exception as e:
            print("[eval] ERROR loading test_loader:", e)
            traceback.print_exc()
            sys.exit(1)

        # --- pick loss function ---
        if loss_name.lower().startswith('bce'):
            loss_fn = nn.BCEWithLogitsLoss()
            is_binary = True
        elif loss_name.lower().startswith('mse'):
            loss_fn = nn.MSELoss()
            is_binary = False
        elif loss_name.lower().startswith('cross'):
            loss_fn = nn.CrossEntropyLoss()
            is_binary = False
        else:
            loss_fn = nn.BCEWithLogitsLoss()
            is_binary = True

        # metrics accumulators
        total_loss = 0.0
        total_n = 0
        tp = 0
        fp = 0
        tn = 0
        fn = 0
        # For multiclass: compute accuracy only; precision/recall/f1 computed with macro averaging
        per_class_tp = {}
        per_class_pred = {}
        per_class_true = {}

        with torch.no_grad():
            for batch in test_iter:
                try:
                    xb, yb = batch
                except Exception:
                    # try if single tensor (features only)
                    continue
                xb = xb.to(device).float()
                yb = yb.to(device)
                out = model(xb)
                # handle shapes
                # if BCEWithLogitsLoss and single output, out may be (B,1) or (B,)
                if isinstance(loss_fn, nn.BCEWithLogitsLoss):
                    # ensure yb is float
                    yb_f = yb.float()
                    if out.dim() > yb_f.dim():
                        out_proc = out.squeeze(-1)
                    else:
                        out_proc = out
                    loss = loss_fn(out_proc, yb_f)
                    probs = torch.sigmoid(out_proc)
                    preds = (probs >= threshold).long()
                    trues = yb.long()
                    # accumulate binary counts
                    for p, t in zip(preds.view(-1).cpu().numpy(), trues.view(-1).cpu().numpy()):
                        if p == 1 and t == 1:
                            tp += 1
                        elif p == 1 and t == 0:
                            fp += 1
                        elif p == 0 and t == 0:
                            tn += 1
                        elif p == 0 and t == 1:
                            fn += 1
                    total_n += yb.numel()
                    total_loss += loss.item() * yb.numel()
                elif isinstance(loss_fn, nn.CrossEntropyLoss):
                    # out: (B, C); yb: (B,) with class indices
                    loss = loss_fn(out, yb.long())
                    preds = torch.argmax(out, dim=1)
                    trues = yb.long()
                    total_n += yb.size(0)
                    total_loss += loss.item() * yb.size(0)
                    # per-class counts
                    for p, t in zip(preds.cpu().numpy(), trues.cpu().numpy()):
                        per_class_pred[p] = per_class_pred.get(p, 0) + 1
                        per_class_true[t] = per_class_true.get(t, 0) + 1
                        if p == t:
                            per_class_tp[p] = per_class_tp.get(p, 0) + 1
                else:
                    # fallback: treat as regression or single-output
                    try:
                        yb_f = yb.float()
                        if out.dim() > yb_f.dim():
                            out_proc = out.squeeze(-1)
                        else:
                            out_proc = out
                        loss = loss_fn(out_proc, yb_f)
                        total_n += yb.numel()
                        total_loss += loss.item() * yb.numel()
                    except Exception as e:
                        # cannot handle this batch
                        continue

        # compute final metrics
        metrics = {}
        if total_n > 0:
            avg_loss = total_loss / float(total_n)
        else:
            avg_loss = None
        metrics['loss'] = avg_loss if avg_loss is not None else None

        if isinstance(loss_fn, nn.BCEWithLogitsLoss):
            # binary metrics
            precision = (tp / (tp + fp)) if (tp + fp) > 0 else 0.0
            recall = (tp / (tp + fn)) if (tp + fn) > 0 else 0.0
            accuracy = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else 0.0
            f1 = (2 * precision * recall / (precision + recall)) if (precision + recall) > 0 else 0.0
            metrics.update({
                "accuracy": accuracy,
                "precision": precision,
                "recall": recall,
                "f1_score": f1
            })
        elif isinstance(loss_fn, nn.CrossEntropyLoss):
            # compute macro precision/recall/f1 from per_class counts
            class_ids = sorted(set(list(per_class_true.keys()) + list(per_class_pred.keys())))
            precisions = []
            recalls = []
            f1s = []
            total_correct = sum(per_class_tp.get(c, 0) for c in class_ids)
            total_pred = sum(per_class_pred.values())
            total_true = sum(per_class_true.values())
            accuracy = total_correct / float(total_true) if total_true > 0 else 0.0
            for c in class_ids:
                tp_c = per_class_tp.get(c, 0)
                pred_c = per_class_pred.get(c, 0)
                true_c = per_class_true.get(c, 0)
                p_c = tp_c / pred_c if pred_c > 0 else 0.0
                r_c = tp_c / true_c if true_c > 0 else 0.0
                f_c = (2 * p_c * r_c / (p_c + r_c)) if (p_c + r_c) > 0 else 0.0
                precisions.append(p_c)
                recalls.append(r_c)
                f1s.append(f_c)
            precision = float(np.mean(precisions)) if precisions else 0.0
            recall = float(np.mean(recalls)) if recalls else 0.0
            f1 = float(np.mean(f1s)) if f1s else 0.0
            metrics.update({
                "accuracy": float(accuracy),
                "precision": float(precision),
                "recall": float(recall),
                "f1_score": float(f1)
            })
        else:
            # regression / fallback: report loss and NaNs for classification metrics
            metrics.update({
                "accuracy": None,
                "precision": None,
                "recall": None,
                "f1_score": None
            })

        # ensure floats are regular python floats
        def norm(v):
            if v is None:
                return None
            try:
                return float(v)
            except:
                return v

        metrics = {k: (norm(v) if not isinstance(v, (dict, list)) else v) for k, v in metrics.items()}

        try:
            atomic_write_json(args.metrics_json, metrics)
            print("[eval] Wrote metrics to:", args.metrics_json)
            print("[eval] Metrics:", metrics)
        except Exception as e:
            print("[eval] ERROR writing metrics:", e)
            traceback.print_exc()
            sys.exit(1)
    args:
      - --test_loader
      - {inputPath: test_loader}
      - --model_config
      - {inputValue: model_config}
      - --trained_model
      - {inputPath: trained_model}
      - --metrics_json
      - {outputPath: metrics_json}
